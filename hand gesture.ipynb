{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3837ec88-0cec-468d-96b3-cf0717686ad4",
   "metadata": {},
   "source": [
    "# develop Hand Gesture recongnition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1bc75f-7ad9-4bea-b778-bb24f421dd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.15.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\91988\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\91988\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abbe6ec-b52d-4061-bcdb-2fdb9bc6bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953f6d6e-1465-43fe-99ca-31340d3d07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab4d9c2-e310-4be9-bba4-d5262069c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20a0122-753a-49ab-8a69-17f43c427898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3e69157-7368-404e-ba3e-3b6a829ba4e0",
   "metadata": {},
   "source": [
    "#Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import of keras model and hidden layers for our convolutional network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "#Image handling libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize a list of paths for images\n",
    "imagepaths = []\n",
    "\n",
    "import os\n",
    "\n",
    "# Specify the path to your local directory containing images\n",
    "local_directory = 'C:/Users/91988/Desktop/hand gesture model/archive (2)/leapGestRecog'\n",
    "\n",
    "for filename in os.listdir(local_directory):\n",
    "    path = os.path.join(local_directory, filename)\n",
    "    if path.endswith(\".png\"):\n",
    "        imagepaths.append(path)\n",
    "\n",
    "print(len(imagepaths))\n",
    "\n",
    "#print(imagepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a92eca-1d8f-448f-8298-0ba775e85857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function that plots the image selected from a path\n",
    "\n",
    "def img_plot(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    #convert to RGB space\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #check the shape of the image\n",
    "    print(\"Shape of the image is \", img_rgb.shape)\n",
    "    #Display the image\n",
    "    plt.grid(False)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Height\")\n",
    "    plt.title(\"Image \" + img_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d9ddebc-464f-4fa9-95df-3068019125f4",
   "metadata": {},
   "source": [
    "# Make the test train split\n",
    "threshold = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = threshold, random_state = 42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "047aa277-114c-4d8b-a1e9-f02b5953f0a1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a CNN Sequential Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5,5), activation = 'relu', input_shape=(128,128,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb838cf-73aa-4b0b-9e46-9703819aa5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model configuration for training purpose\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0da07fda-7b72-40ff-83e4-7b6ff67510d7",
   "metadata": {},
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, \n",
    "         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4473a3a6-6931-487f-9848-98de4ae762d6",
   "metadata": {},
   "source": [
    "model.save('handgesturerecog_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630c86a-a81e-485e-804b-b6e66b95e914",
   "metadata": {},
   "source": [
    "# TESTING THE MODEL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31b7ba37-cd51-4057-aeb1-b095fca66cf6",
   "metadata": {},
   "source": [
    "#calculate loss and accuracy on test data\n",
    "\n",
    "tLoss, tAccuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(tAccuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "549ee92f-f54a-4fec-b1c1-6cc1f7f93d75",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Making predictions on test data\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2fa1c46-cff5-4aaa-a5f9-521cc239efd8",
   "metadata": {},
   "source": [
    "#Lets compare the predicted value with actual label value\n",
    "# Ideally both prediction[0] and y_test[0] should be same\n",
    "np.argmax(prediction[0]), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ab152e-850d-49b4-a3d0-4eed1c9bfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images and labels for validation purposes\n",
    "def validate_gestures(predictions_array, true_label_array, img_array):\n",
    "  # Array for pretty printing and then figure size\n",
    "  class_names = [\"down\", \"palm\", \"l\", \"fist\", \"fist_moved\", \"thumb\", \"index\", \"ok\", \"palm_moved\", \"c\"] \n",
    "  plt.figure(figsize=(15,5))\n",
    "  \n",
    "  for i in range(1, 10):\n",
    "    # Just assigning variables\n",
    "    prediction = predictions_array[i]\n",
    "    true_label = true_label_array[i]\n",
    "    img = img_array[i]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Plot in a good way\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction\n",
    "    \n",
    "    # Change color of title based on good prediction or not\n",
    "    if predicted_label == true_label:\n",
    "      color = 'blue'\n",
    "    else:\n",
    "      color = 'red'\n",
    "\n",
    "    plt.xlabel(\"Predicted: {} {:2.0f}% (Actual: {})\".format(class_names[predicted_label],\n",
    "                                  100*np.max(prediction),\n",
    "                                  class_names[true_label]),\n",
    "                                  color=color)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ec4adda-ef58-42c8-896c-1d448cd6036d",
   "metadata": {},
   "source": [
    "# Plot testing based on predictions and their actual values\n",
    "validate_gestures(prediction, y_test, X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ab91b25-6952-4cec-83d1-cdc485fac732",
   "metadata": {},
   "source": [
    "# Plot testing based on predictions and their actual values\n",
    "validate_gestures(prediction, y_test, X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3694bcfc-1caf-42cc-a7cf-173e5da2234d",
   "metadata": {},
   "source": [
    "#Create a Confusion Matrix for Evaluation\n",
    "# H = Horizontal\n",
    "# V = Vertical\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
    "             columns=[\"Predicted Thumb Down\", \"Predicted Palm (H)\", \"Predicted L\", \"Predicted Fist (H)\", \"Predicted Fist (V)\", \"Predicted Thumbs up\", \"Predicted Index\", \"Predicted OK\", \"Predicted Palm (V)\", \"Predicted C\"],\n",
    "             index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5767df-41c9-4893-92ac-1e98a1f7164d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
